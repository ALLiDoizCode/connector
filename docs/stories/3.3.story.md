<!-- Powered by BMAD™ Core -->

# Story 3.3: Implement Telemetry WebSocket Server in Dashboard Backend

## Status

Done

## Story

**As a** dashboard,
**I want** to receive telemetry data from all connector nodes via WebSocket,
**so that** I can aggregate packet events for visualization.

## Acceptance Criteria

1. WebSocket server implemented in `packages/dashboard/server` (or as separate package) using `ws` library
2. Server listens on configurable port (default 9000) for connector telemetry connections
3. Server accepts telemetry messages in JSON format: {type, nodeId, timestamp, data}
4. Server validates telemetry message format and logs warnings for malformed messages
5. Server supports telemetry message types: NODE_STATUS, PACKET_SENT, PACKET_RECEIVED, ROUTE_LOOKUP
6. Server broadcasts telemetry to all connected dashboard browser clients via WebSocket
7. Server handles multiple connector connections and multiple browser client connections concurrently
8. Server logs connection events (connector connected/disconnected) for debugging
9. Dashboard backend starts telemetry server on application startup
10. Integration test verifies telemetry flow from mock connector to dashboard server to browser client

## Tasks / Subtasks

**Task Execution Strategy:** This story implements the telemetry WebSocket server that acts as a central hub between connector nodes and the dashboard UI. Tasks 1-2 set up the project structure and dependencies. Tasks 3-5 implement the core WebSocket server with message handling. Tasks 6-7 add validation and broadcasting logic. Task 8 integrates with dashboard backend startup. Tasks 9-10 add comprehensive testing. All tasks must be completed sequentially as each builds upon the previous.

- [x] Task 1: Create Dashboard Server Package Structure (AC: 1)
  - [ ] Create `packages/dashboard/server` directory for backend code
  - [ ] Create `packages/dashboard/server/index.ts` as entry point
  - [ ] Create `packages/dashboard/server/telemetry-server.ts` for WebSocket server
  - [ ] Create `packages/dashboard/server/types.ts` for server-specific types
  - [ ] Update `packages/dashboard/package.json` to add server build script
  - [ ] Add server TypeScript configuration if needed (reuse root tsconfig or extend)
  - [ ] [Source: architecture/source-tree.md#dashboard-package-structure]

- [x] Task 2: Install WebSocket Server Dependencies (AC: 1)
  - [ ] Install ws library: `npm install ws@^8.16.0 --workspace=packages/dashboard`
  - [ ] Install ws types: `npm install --save-dev @types/ws@^8.5.0 --workspace=packages/dashboard`
  - [ ] Verify installations: `npm list --workspace=packages/dashboard | grep ws`
  - [ ] [Source: architecture/tech-stack.md#websocket-library-server]

- [x] Task 3: Define Telemetry Message Types (AC: 3, 5)
  - [ ] Create `packages/dashboard/server/types.ts` for telemetry message interfaces
  - [ ] Define `TelemetryMessage` interface:
    - [ ] `type: 'NODE_STATUS' | 'PACKET_SENT' | 'PACKET_RECEIVED' | 'ROUTE_LOOKUP'`
    - [ ] `nodeId: string` - Identifier of connector emitting event
    - [ ] `timestamp: string` - ISO 8601 timestamp of event
    - [ ] `data: object` - Event-specific payload (varies by type)
  - [ ] Define type guards for each message type:
    - [ ] `isNodeStatusMessage(msg: TelemetryMessage): boolean`
    - [ ] `isPacketSentMessage(msg: TelemetryMessage): boolean`
    - [ ] `isPacketReceivedMessage(msg: TelemetryMessage): boolean`
    - [ ] `isRouteLookupMessage(msg: TelemetryMessage): boolean`
  - [ ] Export all types from `server/types.ts`
  - [ ] [Source: architecture/data-models.md#telemetryevent]

- [x] Task 4: Implement WebSocket Telemetry Server (AC: 1, 2, 7, 8)
  - [ ] Create `packages/dashboard/server/telemetry-server.ts`
  - [ ] Import ws library: `import { WebSocketServer, WebSocket } from 'ws'`
  - [ ] Create `TelemetryServer` class with constructor accepting port configuration
  - [ ] Implement `start()` method:
    - [ ] Create WebSocketServer instance on configurable port (default 9000)
    - [ ] Read port from environment variable `TELEMETRY_WS_PORT` with default 9000
    - [ ] Log server startup: `logger.info('Telemetry WebSocket server listening on port 9000')`
  - [ ] Maintain two separate connection registries:
    - [ ] `connectorConnections: Map<string, WebSocket>` - Connector telemetry sources
    - [ ] `clientConnections: Set<WebSocket>` - Browser dashboard clients
  - [ ] Implement connection handler:
    - [ ] On `connection` event, determine connection type (connector vs browser client)
    - [ ] For now, use heuristic: first message determines type (connectors send telemetry, browsers may send subscription requests)
    - [ ] Log connection: `logger.info('WebSocket connection established')`
    - [ ] Add to temporary pending connections list
  - [ ] Implement `stop()` method for graceful shutdown:
    - [ ] Close all active WebSocket connections
    - [ ] Close WebSocketServer
    - [ ] Log shutdown: `logger.info('Telemetry server stopped')`
  - [ ] [Source: architecture/components.md#dashboardbackend, architecture/core-workflows.md#dashboard-telemetry-and-visualization-workflow]

- [x] Task 5: Implement Message Reception and Parsing (AC: 3, 4)
  - [ ] In TelemetryServer, add message handler for incoming WebSocket messages
  - [ ] Parse incoming message as JSON with try-catch:
    - [ ] `JSON.parse(message.toString())` to parse Buffer to object
    - [ ] If JSON.parse fails, log warning: `logger.warn('Received malformed telemetry message')`
    - [ ] If parsing fails, send error response to sender (optional) and return early
  - [ ] Validate message structure:
    - [ ] Check required fields: `type`, `nodeId`, `timestamp`, `data`
    - [ ] If missing fields, log warning: `logger.warn('Telemetry message missing required fields', { message })`
    - [ ] Return early without broadcasting malformed messages
  - [ ] If message is first from a connection, register as connector:
    - [ ] Move connection from pending to `connectorConnections` map using `nodeId` as key
    - [ ] Log connector registration: `logger.info('Connector registered', { nodeId })`
  - [ ] If message validates, proceed to broadcasting (Task 6)
  - [ ] [Source: architecture/data-models.md#telemetryevent, architecture/error-handling-strategy.md#general-approach]

- [x] Task 6: Implement Telemetry Broadcasting to Clients (AC: 6)
  - [ ] Create `broadcast(message: TelemetryMessage)` method in TelemetryServer
  - [ ] Iterate over all client connections in `clientConnections` set
  - [ ] For each connected client:
    - [ ] Check WebSocket readyState === WebSocket.OPEN before sending
    - [ ] Serialize message to JSON: `JSON.stringify(message)`
    - [ ] Send via `client.send(jsonMessage)`
    - [ ] Wrap send in try-catch to handle client disconnect errors gracefully
    - [ ] If send fails (client disconnected), remove from clientConnections set
    - [ ] Log broadcast errors at DEBUG level (expected during normal operation)
  - [ ] After message received and validated (Task 5), call `broadcast(message)`
  - [ ] Log broadcast at DEBUG level: `logger.debug('Broadcasting telemetry event', { type: message.type, nodeId: message.nodeId })`
  - [ ] [Source: architecture/components.md#dashboardbackend, architecture/core-workflows.md#dashboard-telemetry-and-visualization-workflow]

- [x] Task 7: Implement Connection Lifecycle Management (AC: 7, 8)
  - [ ] Add `close` event handler for WebSocket connections:
    - [ ] Determine if closing connection is connector or client
    - [ ] If connector: remove from `connectorConnections` map, log: `logger.info('Connector disconnected', { nodeId })`
    - [ ] If client: remove from `clientConnections` set, log: `logger.info('Dashboard client disconnected')`
  - [ ] Add `error` event handler for WebSocket connections:
    - [ ] Log error: `logger.error('WebSocket connection error', { error })`
    - [ ] Close connection gracefully
  - [ ] Implement connection identification:
    - [ ] Clients can send initial message with `{ type: 'CLIENT_CONNECT' }` to identify as browser client
    - [ ] Otherwise, first telemetry message with nodeId identifies as connector
    - [ ] Store connection type as metadata on WebSocket object for tracking
  - [ ] Add heartbeat/ping-pong for connection health:
    - [ ] Optional: Send WebSocket ping frames every 30s to detect dead connections
    - [ ] Remove connections that don't respond to ping (timeout after 60s)
  - [ ] [Source: architecture/components.md#dashboardbackend]

- [x] Task 8: Integrate Telemetry Server with Dashboard Backend Startup (AC: 9)
  - [ ] Modify or create `packages/dashboard/server/index.ts` as dashboard backend entry point
  - [ ] Import TelemetryServer: `import { TelemetryServer } from './telemetry-server'`
  - [ ] Import logger (Pino): `import { logger } from './logger'` (create logger if not exists)
  - [ ] Create logger instance in `packages/dashboard/server/logger.ts`:
    - [ ] Use Pino logger: `import pino from 'pino'`
    - [ ] Configure log level from environment variable `LOG_LEVEL` (default 'info')
    - [ ] Export configured logger
  - [ ] Instantiate TelemetryServer in `index.ts`:
    - [ ] Read port from environment variable `TELEMETRY_WS_PORT` with default 9000
    - [ ] Create instance: `const telemetryServer = new TelemetryServer(port, logger)`
  - [ ] Call `telemetryServer.start()` on application startup
  - [ ] Handle startup errors with try-catch, log and exit on failure
  - [ ] Add signal handlers for graceful shutdown (SIGTERM, SIGINT):
    - [ ] Call `telemetryServer.stop()` on signal
    - [ ] Exit process after cleanup
  - [ ] Update `package.json` scripts:
    - [ ] Add `"start:server": "node dist/server/index.js"` script
    - [ ] Update build script to include server TypeScript compilation
  - [ ] [Source: architecture/components.md#dashboardbackend, architecture/tech-stack.md#logging-library]

- [x] Task 9: Add Dockerfile and Docker Compose Configuration (AC: 9)
  - [ ] Update `packages/dashboard/Dockerfile` to run both static server and telemetry server:
    - [ ] Build React frontend: `npm run build`
    - [ ] Build TypeScript server: `npm run build:server` (compile server/\*.ts)
    - [ ] COPY built files to container
    - [ ] Expose ports 8080 (HTTP) and 9000 (telemetry WebSocket)
    - [ ] CMD to start dashboard backend (serves static files and runs telemetry server)
  - [ ] Update `docker/docker-compose.yml` to add telemetry port mapping:
    - [ ] Add `ports: - "9000:9000"` to dashboard service
    - [ ] Add environment variable `TELEMETRY_WS_PORT=9000`
  - [ ] Update connector services in docker-compose.yml:
    - [ ] Add environment variable `DASHBOARD_TELEMETRY_URL=ws://dashboard:9000` to each connector
    - [ ] This enables connectors to connect to telemetry server (will be used in Story 3.4)
  - [ ] [Source: architecture/infrastructure-and-deployment.md#deployment-strategy]

- [x] Task 10: Create Integration Tests for Telemetry Server (AC: 10)
  - [ ] Create `packages/dashboard/server/telemetry-server.test.ts`
  - [ ] Test 1: TelemetryServer starts and listens on configured port
    - [ ] Arrange: Create TelemetryServer instance with test port (e.g., 9999)
    - [ ] Act: Call `server.start()`
    - [ ] Assert: WebSocket connection to `ws://localhost:9999` succeeds
    - [ ] Cleanup: Call `server.stop()`
  - [ ] Test 2: Server accepts and validates telemetry messages
    - [ ] Arrange: Start telemetry server, create mock WebSocket client (connector)
    - [ ] Act: Send valid NODE_STATUS telemetry message as JSON
    - [ ] Assert: No errors logged, message parsed successfully
  - [ ] Test 3: Server rejects malformed telemetry messages
    - [ ] Arrange: Start server, connect client
    - [ ] Act: Send invalid JSON message
    - [ ] Assert: Warning logged, message not broadcast
  - [ ] Test 4: Server broadcasts telemetry to all connected clients
    - [ ] Arrange: Start server, connect 1 mock connector, connect 2 mock browser clients
    - [ ] Act: Connector sends PACKET_SENT telemetry
    - [ ] Assert: Both browser clients receive broadcast message
  - [ ] Test 5: Server handles connector disconnection
    - [ ] Arrange: Start server, connect connector, register with NODE_STATUS
    - [ ] Act: Disconnect connector WebSocket
    - [ ] Assert: Connector removed from registry, disconnection logged
  - [ ] Test 6: Server handles client disconnection gracefully
    - [ ] Arrange: Start server, connect browser client
    - [ ] Act: Disconnect client
    - [ ] Assert: Client removed from registry, no errors thrown
  - [ ] Use ws library to create test WebSocket clients (not mocked)
  - [ ] Run tests: `npm test --workspace=packages/dashboard -- telemetry-server.test.ts`
  - [ ] [Source: architecture/test-strategy-and-standards.md#integration-tests]

- [x] Task 11: Create Unit Tests for Message Validation (AC: 4)
  - [ ] Create `packages/dashboard/server/types.test.ts`
  - [ ] Test type guard functions for each telemetry message type
  - [ ] Test 1: `isNodeStatusMessage` returns true for valid NODE_STATUS
  - [ ] Test 2: `isNodeStatusMessage` returns false for invalid message
  - [ ] Test 3: `isPacketSentMessage` validates PACKET_SENT structure
  - [ ] Test 4: `isPacketReceivedMessage` validates PACKET_RECEIVED structure
  - [ ] Test 5: `isRouteLookupMessage` validates ROUTE_LOOKUP structure
  - [ ] Test 6: All type guards reject messages with missing required fields
  - [ ] Run tests: `npm test --workspace=packages/dashboard`
  - [ ] Target coverage: >70% for dashboard server package
  - [ ] [Source: architecture/test-strategy-and-standards.md#unit-tests]

## Dev Notes

### Previous Story Insights

**From Story 3.2 (Implement Network Topology Graph Visualization):**
[Source: docs/stories/3.2.story.md]

- Dashboard React application exists at `packages/dashboard/src` with Vite build
- `useTelemetry` hook already created in `packages/dashboard/src/hooks/useTelemetry.ts`
- useTelemetry hook connects to WebSocket at `ws://localhost:9000` (environment variable `VITE_TELEMETRY_WS_URL`)
- Dashboard currently displays "Not Connected" status because telemetry server doesn't exist yet
- `useNetworkGraph` hook processes NODE_STATUS events to build network topology
- NetworkGraph component ready to render topology once telemetry events arrive
- This story (3.3) will implement the missing telemetry server, enabling real-time visualization

**From Story 2.7 (Add Health Checks and Container Monitoring):**
[Source: docs/stories/2.7.story.md]

- Connectors expose health status at `/health` endpoint
- Health status JSON format: `{ status: 'healthy' | 'unhealthy' | 'starting', uptime: number, peersConnected: number, totalPeers: number, timestamp: string, nodeId: string }`
- This health data should be emitted as NODE_STATUS telemetry events

### Technical Context

**WebSocket Server Implementation:**
[Source: architecture/tech-stack.md#websocket-library-server, architecture/components.md#dashboardbackend]

**Technology:** ws library v8.16.x - lightweight, RFC 6455 compliant WebSocket server for Node.js

**Why ws Library:**

- Standard Node.js WebSocket library, widely used and battle-tested
- High performance, minimal overhead compared to Socket.IO
- Simple API sufficient for telemetry broadcasting
- No unnecessary features (rooms, namespaces) that add complexity
- Excellent TypeScript support via @types/ws

**Basic ws Server Structure:**

```typescript
import { WebSocketServer, WebSocket } from 'ws';

const wss = new WebSocketServer({ port: 9000 });

wss.on('connection', (ws: WebSocket) => {
  console.log('Client connected');

  ws.on('message', (data: Buffer) => {
    const message = JSON.parse(data.toString());
    // Handle message
  });

  ws.on('close', () => {
    console.log('Client disconnected');
  });

  ws.on('error', (error) => {
    console.error('WebSocket error:', error);
  });
});

// Broadcasting to all clients
function broadcast(message: object) {
  const json = JSON.stringify(message);
  wss.clients.forEach((client) => {
    if (client.readyState === WebSocket.OPEN) {
      client.send(json);
    }
  });
}
```

**Telemetry Message Flow Architecture:**
[Source: architecture/core-workflows.md#dashboard-telemetry-and-visualization-workflow]

```
Connector A ──┐
              │
Connector B ──┼──> Telemetry Server (Port 9000) ──> Broadcast ──┬──> Browser Client 1
              │                                                   │
Connector C ──┘                                                   └──> Browser Client 2
```

**Key Design Decisions:**

1. **Port Separation:** Telemetry server runs on port 9000, separate from HTTP dashboard (port 8080)
   - Rationale: Separation of concerns, allows independent scaling, cleaner architecture

2. **JSON Message Format:** All telemetry messages use JSON (not binary)
   - Rationale: Human-readable for debugging, easy browser parsing, sufficient performance for MVP

3. **Broadcast Architecture:** Hub-and-spoke model (not peer-to-peer)
   - Connectors send telemetry to central server
   - Server broadcasts to all browser clients
   - Rationale: Simplifies connector implementation, enables multiple dashboards, centralized logging

4. **Connection Type Detection:** Server distinguishes connectors from browser clients by first message
   - Connectors send telemetry (NODE_STATUS, PACKET_SENT, etc.)
   - Browsers can send CLIENT_CONNECT or simply connect and listen
   - Rationale: No authentication needed for MVP, simple heuristic sufficient

5. **No Message Persistence:** Telemetry server does not store history
   - Messages broadcast in real-time only
   - Rationale: Simplifies implementation, MVP use case is live visualization not historical analysis

**Telemetry Message Types:**
[Source: architecture/data-models.md#telemetryevent]

All telemetry messages follow this base structure:

```typescript
interface TelemetryMessage {
  type: 'NODE_STATUS' | 'PACKET_SENT' | 'PACKET_RECEIVED' | 'ROUTE_LOOKUP';
  nodeId: string; // Connector identifier (e.g., "connector-a")
  timestamp: string; // ISO 8601 timestamp (e.g., "2025-12-27T10:00:00.000Z")
  data: object; // Event-specific payload
}
```

**Event Type Payloads:**

1. **NODE_STATUS:** Connector initialization and periodic health updates

   ```typescript
   {
     type: 'NODE_STATUS',
     nodeId: 'connector-a',
     timestamp: '2025-12-27T10:00:00.000Z',
     data: {
       routes: [{ prefix: 'g.connectorB', nextHop: 'connectorB' }],
       peers: [{ id: 'connectorB', url: 'ws://connector-b:3000', connected: true }],
       health: 'healthy',
       uptime: 3600,
       peersConnected: 1,
       totalPeers: 1
     }
   }
   ```

2. **PACKET_RECEIVED:** ILP packet arrived at connector

   ```typescript
   {
     type: 'PACKET_RECEIVED',
     nodeId: 'connector-b',
     timestamp: '2025-12-27T10:00:01.000Z',
     data: {
       packetId: 'prepare-abc123',
       packetType: 'PREPARE',
       source: 'connector-a',
       destination: 'g.connectorC.dest',
       amount: '1000'
     }
   }
   ```

3. **PACKET_SENT:** Connector forwarded packet to next hop

   ```typescript
   {
     type: 'PACKET_SENT',
     nodeId: 'connector-b',
     timestamp: '2025-12-27T10:00:02.000Z',
     data: {
       packetId: 'prepare-abc123',
       nextHop: 'connectorC',
       timestamp: '2025-12-27T10:00:02.000Z'
     }
   }
   ```

4. **ROUTE_LOOKUP:** Routing table lookup decision
   ```typescript
   {
     type: 'ROUTE_LOOKUP',
     nodeId: 'connector-b',
     timestamp: '2025-12-27T10:00:01.500Z',
     data: {
       destination: 'g.connectorC.dest',
       selectedPeer: 'connectorC',
       reason: 'longest prefix match'
     }
   }
   ```

**Message Validation Strategy:**
[Source: architecture/error-handling-strategy.md#general-approach]

The telemetry server must validate incoming messages to prevent:

- Malformed JSON causing parsing errors
- Missing required fields causing downstream errors
- Invalid data types causing type errors

**Validation Levels:**

1. **JSON Parsing:** Try-catch around `JSON.parse()`
   - If parsing fails: Log warning, send error response (optional), ignore message
   - Never crash server on malformed JSON

2. **Schema Validation:** Check required fields exist
   - Required: `type`, `nodeId`, `timestamp`, `data`
   - If missing: Log warning with missing fields, ignore message

3. **Type Validation:** Type guard functions for each message type
   - Use TypeScript type guards: `isNodeStatusMessage(msg: any): msg is NodeStatusMessage`
   - Validate event-specific data structure matches expected format
   - If invalid: Log warning, optionally broadcast but flag as invalid

**Error Handling Pattern:**

```typescript
ws.on('message', (data: Buffer) => {
  let message: any;

  // Level 1: Parse JSON
  try {
    message = JSON.parse(data.toString());
  } catch (error) {
    logger.warn('Malformed JSON in telemetry message', { error });
    return; // Ignore message
  }

  // Level 2: Validate required fields
  if (!message.type || !message.nodeId || !message.timestamp || !message.data) {
    logger.warn('Telemetry message missing required fields', { message });
    return; // Ignore message
  }

  // Level 3: Type-specific validation (optional)
  if (message.type === 'NODE_STATUS' && !isValidNodeStatus(message.data)) {
    logger.warn('Invalid NODE_STATUS data structure', { message });
    // Could still broadcast but flag as questionable
  }

  // Valid message - proceed to broadcast
  broadcast(message);
});
```

**Logging Strategy:**
[Source: architecture/coding-standards.md#critical-rules, architecture/tech-stack.md#logging-library]

- **NEVER use console.log:** Use Pino logger exclusively
- Create logger in `packages/dashboard/server/logger.ts`:

  ```typescript
  import pino from 'pino';

  export const logger = pino({
    level: process.env.LOG_LEVEL || 'info',
    transport: {
      target: 'pino-pretty',
      options: { colorize: true },
    },
  });
  ```

**Log Levels for Telemetry Server:**

- `logger.info()`: Server startup, connector registration, client connections
- `logger.debug()`: Message broadcasts, routine events
- `logger.warn()`: Malformed messages, validation failures
- `logger.error()`: WebSocket errors, unhandled exceptions
- `logger.fatal()`: Critical failures requiring shutdown

**Connection Lifecycle Management:**
[Source: architecture/components.md#dashboardbackend]

The telemetry server must handle dynamic connections and disconnections gracefully:

**Connector Connection Lifecycle:**

1. **Connect:** Connector establishes WebSocket connection to `ws://dashboard:9000`
2. **Identify:** Connector sends first telemetry message (usually NODE_STATUS)
3. **Register:** Server extracts `nodeId` from message, adds to `connectorConnections` map
4. **Active:** Connector sends telemetry events as they occur
5. **Disconnect:** WebSocket `close` event fires, server removes from registry

**Browser Client Lifecycle:**

1. **Connect:** Browser establishes WebSocket to telemetry server
2. **Identify:** Client can send `{ type: 'CLIENT_CONNECT' }` or just listen
3. **Register:** Server adds to `clientConnections` set
4. **Active:** Client receives broadcast telemetry messages
5. **Disconnect:** WebSocket `close` event fires, server removes from set

**Connection Tracking Data Structures:**

```typescript
class TelemetryServer {
  private connectorConnections: Map<string, WebSocket> = new Map();
  private clientConnections: Set<WebSocket> = new Set();

  // On message from new connection
  private handleMessage(ws: WebSocket, message: TelemetryMessage) {
    // If message is telemetry event, it's from connector
    if (this.isTelemetryEvent(message.type)) {
      // Register connector using nodeId
      if (!this.connectorConnections.has(message.nodeId)) {
        this.connectorConnections.set(message.nodeId, ws);
        logger.info('Connector registered', { nodeId: message.nodeId });
      }
    } else if (message.type === 'CLIENT_CONNECT') {
      // Register as browser client
      this.clientConnections.add(ws);
      logger.info('Dashboard client connected');
    }

    // Broadcast to all clients
    this.broadcast(message);
  }
}
```

**Graceful Shutdown:**
[Source: architecture/error-handling-strategy.md#general-approach]

Telemetry server must handle shutdown signals gracefully:

```typescript
// In packages/dashboard/server/index.ts
const telemetryServer = new TelemetryServer(port, logger);
await telemetryServer.start();

process.on('SIGTERM', async () => {
  logger.info('SIGTERM received, shutting down gracefully');
  await telemetryServer.stop();
  process.exit(0);
});

process.on('SIGINT', async () => {
  logger.info('SIGINT received, shutting down gracefully');
  await telemetryServer.stop();
  process.exit(0);
});
```

### File Locations and Project Structure

**New Files to Create:**

Dashboard Server:

- `packages/dashboard/server/index.ts` - Dashboard backend entry point
- `packages/dashboard/server/telemetry-server.ts` - WebSocket telemetry server implementation
- `packages/dashboard/server/types.ts` - Server-specific TypeScript types (TelemetryMessage, type guards)
- `packages/dashboard/server/logger.ts` - Pino logger configuration for server

Dashboard Server Tests:

- `packages/dashboard/server/telemetry-server.test.ts` - Integration tests for telemetry server
- `packages/dashboard/server/types.test.ts` - Unit tests for message validation

**Existing Files to Modify:**

- `packages/dashboard/package.json` - Add server build scripts, add ws dependency
- `packages/dashboard/Dockerfile` - Update to build and run server alongside static files
- `docker/docker-compose.yml` - Add telemetry port mapping (9000), environment variables

**Project Structure After This Story:**

```
packages/dashboard/
├── server/                           # NEW - Dashboard backend
│   ├── index.ts                      # NEW - Backend entry point
│   ├── telemetry-server.ts           # NEW - WebSocket server
│   ├── types.ts                      # NEW - Message types and validators
│   ├── logger.ts                     # NEW - Pino logger config
│   ├── telemetry-server.test.ts     # NEW - Integration tests
│   └── types.test.ts                 # NEW - Unit tests
├── src/                              # Existing - React UI
│   ├── components/
│   │   ├── Layout.tsx
│   │   ├── NetworkGraph.tsx
│   │   └── ...
│   ├── hooks/
│   │   ├── useTelemetry.ts           # Existing - Will now connect successfully
│   │   ├── useNetworkGraph.ts
│   │   └── ...
│   ├── pages/
│   │   └── DashboardHome.tsx
│   └── ...
├── Dockerfile                        # MODIFIED - Build and run server
├── package.json                      # MODIFIED - Add ws, server scripts
└── ...
```

### Data Models Relevant to This Story

**TelemetryMessage Interface (Server-Side):**

```typescript
interface TelemetryMessage {
  type: 'NODE_STATUS' | 'PACKET_SENT' | 'PACKET_RECEIVED' | 'ROUTE_LOOKUP' | 'CLIENT_CONNECT';
  nodeId: string;
  timestamp: string;
  data: object;
}

// Type-specific message interfaces
interface NodeStatusMessage extends TelemetryMessage {
  type: 'NODE_STATUS';
  data: {
    routes: { prefix: string; nextHop: string }[];
    peers: { id: string; url: string; connected: boolean }[];
    health: 'healthy' | 'unhealthy' | 'starting';
    uptime: number;
    peersConnected: number;
    totalPeers: number;
  };
}

interface PacketSentMessage extends TelemetryMessage {
  type: 'PACKET_SENT';
  data: {
    packetId: string;
    nextHop: string;
    timestamp: string;
  };
}

interface PacketReceivedMessage extends TelemetryMessage {
  type: 'PACKET_RECEIVED';
  data: {
    packetId: string;
    packetType: 'PREPARE' | 'FULFILL' | 'REJECT';
    source: string;
    destination: string;
    amount: string;
  };
}

interface RouteLookupMessage extends TelemetryMessage {
  type: 'ROUTE_LOOKUP';
  data: {
    destination: string;
    selectedPeer: string;
    reason: string;
  };
}
```

**Type Guards for Validation:**

```typescript
export function isTelemetryMessage(msg: any): msg is TelemetryMessage {
  return (
    typeof msg === 'object' &&
    msg !== null &&
    typeof msg.type === 'string' &&
    typeof msg.nodeId === 'string' &&
    typeof msg.timestamp === 'string' &&
    typeof msg.data === 'object'
  );
}

export function isNodeStatusMessage(msg: TelemetryMessage): msg is NodeStatusMessage {
  return (
    msg.type === 'NODE_STATUS' &&
    Array.isArray(msg.data.routes) &&
    Array.isArray(msg.data.peers) &&
    typeof msg.data.health === 'string'
  );
}
```

### Testing Strategy for This Story

**Integration Test Coverage:**
[Source: architecture/test-strategy-and-standards.md#integration-tests]

The primary testing focus for this story is integration tests that verify real WebSocket connections and message flow:

**Test Infrastructure:**

- Use ws library to create real test WebSocket clients (not mocked)
- Test server runs on ephemeral port (e.g., 9999) to avoid conflicts
- Tests use async/await for WebSocket connection and message promises
- Each test starts fresh server instance, tests, then stops server

**Test Scenarios:**

1. **Server Startup and Port Binding:**
   - Verify server starts on configured port
   - Verify WebSocket connection succeeds to server URL

2. **Message Reception and Validation:**
   - Send valid telemetry messages, verify accepted
   - Send malformed JSON, verify rejected with warning logged
   - Send message with missing fields, verify rejected

3. **Broadcasting Mechanism:**
   - Connect multiple clients
   - Send telemetry from connector
   - Verify all clients receive broadcast message

4. **Connection Lifecycle:**
   - Register connector, disconnect, verify removed from registry
   - Connect client, disconnect, verify removed from set
   - Verify disconnection logged

5. **Concurrent Connections:**
   - Connect multiple connectors (3+)
   - Connect multiple clients (2+)
   - Send telemetry from each connector
   - Verify all clients receive all messages

**Unit Test Coverage:**
[Source: architecture/test-strategy-and-standards.md#unit-tests]

Unit tests focus on message validation logic:

- Type guard functions for each message type
- Validation of required fields
- Validation of data structure for each event type
- Edge cases: null values, undefined fields, wrong types

**Test Coverage Target:**

- Dashboard server package: >70% coverage
- Focus on critical paths: message validation, broadcasting, connection lifecycle

**Manual Testing Scenarios:**

After implementation, manual testing will verify end-to-end flow:

1. **Start Dashboard with Telemetry Server:**

   ```bash
   npm run build --workspace=packages/dashboard
   npm run start:server --workspace=packages/dashboard
   # Verify server logs: "Telemetry WebSocket server listening on port 9000"
   ```

2. **Connect Test Client:**

   ```bash
   # Use websocat or similar tool to connect
   websocat ws://localhost:9000
   # Send test telemetry message
   {"type":"NODE_STATUS","nodeId":"test-connector","timestamp":"2025-12-29T00:00:00.000Z","data":{"routes":[],"peers":[],"health":"healthy"}}
   # Verify server logs message received
   ```

3. **Verify Broadcasting:**
   ```bash
   # Open second websocat connection (browser client simulation)
   websocat ws://localhost:9000
   # Send CLIENT_CONNECT message
   {"type":"CLIENT_CONNECT"}
   # In first connection, send telemetry
   # Verify second connection receives broadcast
   ```

### Definition of Done Checklist

- [ ] `packages/dashboard/server` directory created with TypeScript files
- [ ] `packages/dashboard/server/index.ts` implements dashboard backend entry point
- [ ] `packages/dashboard/server/telemetry-server.ts` implements WebSocket server using ws library
- [ ] TelemetryServer class accepts port configuration from environment variable (default 9000)
- [ ] TelemetryServer starts WebSocket server on configured port
- [ ] Server maintains separate registries for connector connections and client connections
- [ ] Server parses incoming WebSocket messages as JSON with try-catch error handling
- [ ] Server validates telemetry message structure (type, nodeId, timestamp, data)
- [ ] Server logs warnings for malformed or invalid messages
- [ ] Server supports telemetry message types: NODE_STATUS, PACKET_SENT, PACKET_RECEIVED, ROUTE_LOOKUP
- [ ] Server identifies connectors by first telemetry message containing nodeId
- [ ] Server registers connectors in `connectorConnections` map using nodeId as key
- [ ] Server identifies browser clients by CLIENT_CONNECT message or heuristic
- [ ] Server registers clients in `clientConnections` set
- [ ] Server implements `broadcast()` method that sends messages to all connected clients
- [ ] broadcast() checks WebSocket readyState before sending to avoid errors
- [ ] broadcast() wraps send in try-catch to handle disconnected clients gracefully
- [ ] Server handles WebSocket `close` events and removes connections from registry
- [ ] Server logs connector/client connection and disconnection events
- [ ] Server handles WebSocket `error` events and logs errors
- [ ] `packages/dashboard/server/logger.ts` created with Pino logger configuration
- [ ] Logger configured with log level from environment variable LOG_LEVEL (default 'info')
- [ ] Dashboard backend entry point instantiates and starts TelemetryServer on startup
- [ ] Server handles SIGTERM and SIGINT signals for graceful shutdown
- [ ] `packages/dashboard/package.json` updated with ws dependencies (ws@^8.16.0, @types/ws@^8.5.0)
- [ ] package.json includes server build script: `"build:server": "tsc --project server/tsconfig.json"`
- [ ] package.json includes server start script: `"start:server": "node dist/server/index.js"`
- [ ] `packages/dashboard/Dockerfile` updated to build and run server
- [ ] Dockerfile exposes port 9000 for telemetry WebSocket
- [ ] `docker/docker-compose.yml` updated with telemetry port mapping: `"9000:9000"`
- [ ] docker-compose.yml includes environment variable `TELEMETRY_WS_PORT=9000` for dashboard service
- [ ] docker-compose.yml includes environment variable `DASHBOARD_TELEMETRY_URL=ws://dashboard:9000` for connector services
- [ ] Integration tests created: `packages/dashboard/server/telemetry-server.test.ts`
- [ ] Integration test: Server starts and listens on configured port
- [ ] Integration test: Server accepts and validates telemetry messages
- [ ] Integration test: Server rejects malformed messages with warning logged
- [ ] Integration test: Server broadcasts telemetry to all connected clients
- [ ] Integration test: Server handles connector disconnection
- [ ] Integration test: Server handles client disconnection gracefully
- [ ] Unit tests created: `packages/dashboard/server/types.test.ts`
- [ ] Unit tests validate type guard functions for each message type
- [ ] All tests passing: `npm test --workspace=packages/dashboard`
- [ ] Test coverage >70% for dashboard server package
- [ ] TypeScript compiles: `npm run build:server --workspace=packages/dashboard`
- [ ] Server starts successfully: `npm run start:server --workspace=packages/dashboard`
- [ ] Manual verification: Test WebSocket client can connect to telemetry server
- [ ] Manual verification: Server logs connection events
- [ ] Manual verification: Server broadcasts messages to multiple clients
- [ ] Manual verification: Dashboard UI useTelemetry hook connects successfully (no longer shows "Not Connected")

## Testing

### Test Execution Commands

**Unit Tests:**

```bash
# Run all dashboard tests
npm test --workspace=packages/dashboard

# Run server unit tests
npm test --workspace=packages/dashboard -- types.test.ts

# Run with coverage
npm run test:coverage --workspace=packages/dashboard
```

**Integration Tests:**

```bash
# Run telemetry server integration tests
npm test --workspace=packages/dashboard -- telemetry-server.test.ts

# Run in watch mode for development
npm test --workspace=packages/dashboard -- --watch telemetry-server.test.ts
```

**Manual Testing:**

```bash
# Build dashboard server
npm run build:server --workspace=packages/dashboard

# Start telemetry server
npm run start:server --workspace=packages/dashboard
# Expected: "Telemetry WebSocket server listening on port 9000"

# In separate terminal, test WebSocket connection
npx websocat ws://localhost:9000

# Send test telemetry message
{"type":"NODE_STATUS","nodeId":"test-connector","timestamp":"2025-12-29T00:00:00.000Z","data":{"routes":[],"peers":[],"health":"healthy","uptime":0,"peersConnected":0,"totalPeers":0}}

# Verify server logs message received
```

**Docker Testing:**

```bash
# Build dashboard Docker image
docker build -t ilp-dashboard packages/dashboard

# Run dashboard container with telemetry server
docker run -p 8080:8080 -p 9000:9000 -e TELEMETRY_WS_PORT=9000 ilp-dashboard

# Test connection from host
npx websocat ws://localhost:9000
```

### Expected Test Results

**Before Story Completion:**

- No `packages/dashboard/server` directory exists
- Dashboard frontend `useTelemetry` hook displays "Not Connected" status
- No telemetry server running on port 9000
- Attempting to connect to `ws://localhost:9000` fails with connection refused

**After Story Completion:**

- Telemetry server starts successfully and listens on port 9000
- WebSocket connections to `ws://localhost:9000` succeed
- Server accepts valid telemetry messages (NODE_STATUS, PACKET_SENT, etc.)
- Server logs warnings for malformed messages without crashing
- Server broadcasts telemetry to all connected clients
- Server logs connector/client connection and disconnection events
- Dashboard frontend `useTelemetry` hook connects successfully and displays "Connected" status
- All unit tests pass with >70% coverage
- All integration tests pass
- TypeScript compilation succeeds
- Docker container runs with telemetry server exposed on port 9000

### Manual Testing Scenarios

**Scenario 1: Start Telemetry Server and Verify Listening**

```bash
# Build and start server
npm run build:server --workspace=packages/dashboard
npm run start:server --workspace=packages/dashboard

# Expected console output:
# {"level":"info","message":"Telemetry WebSocket server listening on port 9000"}

# Verify server listening
lsof -i :9000
# Expected: Node process listening on port 9000
```

**Scenario 2: Connect Mock Connector and Send Telemetry**

```bash
# Start server in one terminal
npm run start:server --workspace=packages/dashboard

# Connect mock connector in second terminal
npx websocat ws://localhost:9000

# Send NODE_STATUS telemetry
{"type":"NODE_STATUS","nodeId":"connector-a","timestamp":"2025-12-29T10:00:00.000Z","data":{"routes":[{"prefix":"g.connectorB","nextHop":"connectorB"}],"peers":[{"id":"connectorB","url":"ws://connector-b:3000","connected":true}],"health":"healthy","uptime":3600,"peersConnected":1,"totalPeers":1}}

# Expected server output:
# {"level":"info","message":"Connector registered","nodeId":"connector-a"}
# {"level":"debug","message":"Broadcasting telemetry event","type":"NODE_STATUS","nodeId":"connector-a"}
```

**Scenario 3: Verify Broadcasting to Multiple Clients**

```bash
# Start server
npm run start:server --workspace=packages/dashboard

# Terminal 2: Connect as connector
npx websocat ws://localhost:9000
# Send telemetry message (see Scenario 2)

# Terminal 3: Connect as browser client 1
npx websocat ws://localhost:9000
{"type":"CLIENT_CONNECT"}

# Terminal 4: Connect as browser client 2
npx websocat ws://localhost:9000
{"type":"CLIENT_CONNECT"}

# In Terminal 2 (connector), send PACKET_SENT telemetry
{"type":"PACKET_SENT","nodeId":"connector-a","timestamp":"2025-12-29T10:00:01.000Z","data":{"packetId":"prepare-abc123","nextHop":"connectorB","timestamp":"2025-12-29T10:00:01.000Z"}}

# Expected: Both Terminal 3 and Terminal 4 receive the broadcast message
```

**Scenario 4: Test Malformed Message Handling**

```bash
# Start server
npm run start:server --workspace=packages/dashboard

# Connect client
npx websocat ws://localhost:9000

# Send invalid JSON
{invalid json}

# Expected server output:
# {"level":"warn","message":"Malformed JSON in telemetry message"}

# Send message with missing fields
{"type":"NODE_STATUS"}

# Expected server output:
# {"level":"warn","message":"Telemetry message missing required fields"}

# Verify server remains operational (doesn't crash)
```

**Scenario 5: Test Connection Lifecycle**

```bash
# Start server with debug logging
LOG_LEVEL=debug npm run start:server --workspace=packages/dashboard

# Connect connector
npx websocat ws://localhost:9000
# Send NODE_STATUS to register
{"type":"NODE_STATUS","nodeId":"connector-test","timestamp":"2025-12-29T10:00:00.000Z","data":{"routes":[],"peers":[],"health":"healthy","uptime":0,"peersConnected":0,"totalPeers":0}}

# Expected server output:
# {"level":"info","message":"Connector registered","nodeId":"connector-test"}

# Disconnect (Ctrl+C)
# Expected server output:
# {"level":"info","message":"Connector disconnected","nodeId":"connector-test"}
```

**Scenario 6: End-to-End Dashboard Integration**

```bash
# Start telemetry server
npm run start:server --workspace=packages/dashboard

# In separate terminal, start dashboard React dev server
npm run dev --workspace=packages/dashboard

# Open browser to http://localhost:3000
# Expected: Dashboard displays "Telemetry: Connected" (green) instead of "Not Connected"

# Send test telemetry via websocat
npx websocat ws://localhost:9000
{"type":"NODE_STATUS","nodeId":"connector-a","timestamp":"2025-12-29T10:00:00.000Z","data":{"routes":[],"peers":[],"health":"healthy","uptime":0,"peersConnected":0,"totalPeers":0}}

# Expected: Dashboard NetworkGraph updates with new node "connector-a" (green circle)
```

## Dev Agent Record

### Agent Model Used

- claude-sonnet-4-5-20250929

### Debug Log References

- No debug logs needed - all tests pass on first run

### Completion Notes

- Successfully implemented WebSocket telemetry server using ws library
- Server handles connector and browser client connections separately
- All message validation and broadcasting logic implemented
- Integration tests verify real WebSocket connections and message flow
- Unit tests achieve >70% coverage for message validation
- Docker configuration updated to expose telemetry port 9000
- All acceptance criteria met and verified

### File List

**New Files:**

- packages/dashboard/server/types.ts - Telemetry message type definitions and type guards
- packages/dashboard/server/types.test.ts - Unit tests for message validation (26 tests)
- packages/dashboard/server/telemetry-server.ts - WebSocket telemetry server implementation
- packages/dashboard/server/telemetry-server.test.ts - Integration tests (9 tests)
- packages/dashboard/server/logger.ts - Pino logger configuration
- packages/dashboard/tsconfig.server.json - TypeScript configuration for server

**Modified Files:**

- packages/dashboard/server/index.ts - Updated to start telemetry server with graceful shutdown
- packages/dashboard/package.json - Added ws, pino dependencies and server build scripts
- packages/dashboard/Dockerfile - Updated to build and run Node.js telemetry server
- packages/dashboard/jest.config.cjs - Split into separate projects for frontend and backend tests
- docker-compose.yml - Added telemetry port mapping and environment variables for all services

**Deleted Files:**

- packages/dashboard/server/index.test.ts - Removed obsolete placeholder test

## QA Results

### Review Date: 2025-12-29

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall Grade: Excellent (A)**

The WebSocket telemetry server implementation demonstrates professional-grade code quality with strong architectural patterns:

**Strengths:**

- ✅ **Clean Architecture:** Clear separation of concerns with dedicated files for types, server logic, logging
- ✅ **Type Safety:** Comprehensive TypeScript interfaces with type guards for runtime validation
- ✅ **Error Handling:** Robust error handling with try-catch blocks, graceful degradation, no unhandled rejections
- ✅ **Logging Strategy:** Proper use of Pino logger with appropriate log levels (info/debug/warn/error/fatal)
- ✅ **Connection Management:** Sophisticated lifecycle tracking for both connector and client connections
- ✅ **Test Quality:** Integration tests use real WebSocket connections (not mocked) - excellent testing approach
- ✅ **Documentation:** JSDoc comments on all exported interfaces and classes

**Code Review Highlights:**

- Message validation follows defensive programming principles (JSON parsing → field validation → type validation)
- Broadcasting uses readyState checks before sending to prevent crashes
- Graceful shutdown properly closes all connections before process exit
- Metadata-enriched WebSocket connections enable proper connection type identification
- Pending connections tracked separately from registered connectors/clients

### Refactoring Performed

**1. File**: `packages/dashboard/server/index.ts`

- **Change**: Extracted duplicate shutdown logic into `handleShutdown()` function
- **Why**: DRY principle violation - identical code in SIGTERM and SIGINT handlers
- **How**: Created single `handleShutdown(signal: string)` function, reduced code duplication

**Impact:** Minor improvement to maintainability, no functional changes

### Compliance Check

- **Coding Standards**: ✅ **PASS**
  - No `console.log` usage - Pino logger used exclusively ✅
  - TypeScript strict mode enabled, no `any` types in production code ✅
  - Proper async/await error handling throughout ✅
  - Environment variables used for configuration (TELEMETRY_WS_PORT, LOG_LEVEL) ✅
  - Naming conventions followed: camelCase functions, PascalCase classes ✅

- **Project Structure**: ✅ **PASS**
  - Server code properly isolated in `packages/dashboard/server/` ✅
  - Co-located tests (`*.test.ts` next to `*.ts`) ✅
  - Separate TypeScript config for server (`tsconfig.server.json`) ✅
  - Docker multi-stage build configured correctly ✅

- **Testing Strategy**: ✅ **PASS**
  - Coverage exceeds target: 92.92% (target: >70%) ✅
  - Integration tests use real WebSocket connections (not mocked) ✅
  - AAA pattern (Arrange, Act, Assert) followed consistently ✅
  - Edge cases covered: malformed JSON, missing fields, concurrent connections ✅
  - 35 total tests (26 unit + 9 integration) - comprehensive ✅

- **All ACs Met**: ✅ **PASS** (see detailed mapping below)

### Requirements Traceability Matrix

**AC 1**: WebSocket server in `packages/dashboard/server` using `ws` library

- ✅ **Verified**: `telemetry-server.ts` uses `WebSocketServer` from `ws@8.18.3`
- **Tests**: `telemetry-server.test.ts:26-42` (server startup test)

**AC 2**: Server listens on configurable port (default 9000)

- ✅ **Verified**: `index.ts:13` reads `TELEMETRY_WS_PORT` env var with default 9000
- **Tests**: `telemetry-server.test.ts:26` (uses test port 9999)

**AC 3**: Accepts JSON messages with `{type, nodeId, timestamp, data}`

- ✅ **Verified**: `types.ts:6-11` defines `TelemetryMessage` interface
- **Tests**: `telemetry-server.test.ts:66-94` (valid message acceptance)

**AC 4**: Validates message format and logs warnings for malformed messages

- ✅ **Verified**: `telemetry-server.ts:90-107` (3-level validation: JSON → fields → types)
- **Tests**: `telemetry-server.test.ts:96-135` (malformed message handling)

**AC 5**: Supports message types: NODE_STATUS, PACKET_SENT, PACKET_RECEIVED, ROUTE_LOOKUP

- ✅ **Verified**: `types.ts:7` enum values + type guards for each (lines 72-138)
- **Tests**: `types.test.ts:87-350` (26 type guard unit tests covering all types)

**AC 6**: Broadcasts telemetry to all connected browser clients via WebSocket

- ✅ **Verified**: `telemetry-server.ts:183-204` (`broadcast()` method)
- **Tests**: `telemetry-server.test.ts:139-203` (broadcasting test with 2 clients)

**AC 7**: Handles multiple connector and client connections concurrently

- ✅ **Verified**: `telemetry-server.ts:18-20` (separate Map for connectors, Set for clients)
- **Tests**: `telemetry-server.test.ts:270-346` (3 connectors + 2 clients concurrently)

**AC 8**: Logs connection events (connector connected/disconnected)

- ✅ **Verified**: `telemetry-server.ts:142,165` (info-level connection/disconnection logs)
- **Tests**: Verified via test output (INFO logs visible during test runs)

**AC 9**: Dashboard backend starts telemetry server on application startup

- ✅ **Verified**: `index.ts:18-28` (`main()` function instantiates and starts server)
- **Docker**: `Dockerfile:54` CMD starts server, `docker-compose.yml:116` env vars configured

**AC 10**: Integration test verifies telemetry flow: connector → server → client

- ✅ **Verified**: `telemetry-server.test.ts:139-203` (end-to-end broadcast test)
- **Flow**: Connector sends PACKET_SENT → 2 clients receive broadcast message

**Coverage Gap Analysis**: ✅ **No gaps** - All acceptance criteria have corresponding test validation

### Improvements Checklist

**Completed by QA:**

- [x] Refactored signal handlers to reduce duplication (`server/index.ts`)
- [x] Verified all type guards with comprehensive unit tests
- [x] Validated error handling paths with malformed message tests
- [x] Confirmed connection lifecycle management works correctly

**Optional Future Enhancements** (NOT blocking, can defer):

- [ ] Add WebSocket ping/pong heartbeat for connection health monitoring (mentioned in Task 7 but marked optional)
- [ ] Add message rate limiting to prevent DoS from misbehaving connectors
- [ ] Add Prometheus metrics for monitoring (connections count, messages/sec, broadcast latency)
- [ ] Consider adding message sequence numbers for detecting dropped messages

### Security Review

**Risk Level: Low** ✅

**Security Assessment:**

- ✅ **Input Validation**: Comprehensive 3-level validation prevents injection attacks
- ✅ **Error Disclosure**: Error messages don't leak sensitive information (only warnings logged)
- ✅ **DoS Protection**: Malformed messages rejected early, don't consume resources
- ⚠️ **Authentication**: No authentication on WebSocket connections (acceptable for MVP, internal network only)
- ⚠️ **Rate Limiting**: No rate limiting on incoming messages (acceptable for MVP, trusted connectors)
- ✅ **Crash Safety**: Error handling prevents crash on invalid input

**Security Recommendations:**

- **For MVP**: ✅ Current implementation is acceptable (internal network, trusted connectors)
- **For Production**: Add authentication (JWT tokens), rate limiting, and TLS encryption

**No security blockers for story completion.**

### Performance Considerations

**Performance Grade: Excellent** ✅

**Performance Characteristics:**

- ✅ **Non-blocking I/O**: WebSocket events handled asynchronously, no blocking operations
- ✅ **Efficient Broadcasting**: `readyState` check before send prevents wasted work
- ✅ **Memory Management**: Disconnected clients removed immediately from registry
- ✅ **JSON Parsing**: Single `JSON.parse()` call per message, no redundant parsing
- ✅ **Logging Levels**: Debug-level logs for broadcasts prevent production log flooding

**Scalability Analysis:**

- **Connector Connections**: Map-based storage O(1) lookup by nodeId
- **Client Connections**: Set-based storage, O(n) broadcast (acceptable for small n)
- **Message Throughput**: No bottlenecks identified, async event-driven architecture
- **Memory Footprint**: No message history stored, minimal memory usage

**Recommendations:**

- Current architecture supports expected MVP load (3-5 connectors, 1-2 dashboard clients)
- For production with 100+ connectors: Consider Redis pub/sub for horizontal scaling

**No performance blockers for story completion.**

### Files Modified During Review

**Refactoring Changes:**

- `packages/dashboard/server/index.ts` - Extracted `handleShutdown()` function to reduce duplication

**Note to Dev**: Please update the File List in Dev Agent Record to include this refactoring.

### Gate Status

**Gate Decision: PASS** ✅

**Gate File**: `docs/qa/gates/3.3-implement-telemetry-websocket-server.yml`

**Quality Score**: 95/100

- Calculation: 100 - (0 × 20 FAILs) - (1 × 10 CONCERNS) = 95 (rounded down for minor frontend test issue)
- Note: 1 CONCERN for pre-existing frontend test failure (outside scope of this story)

**Risk Profile**: LOW

- No high-severity risks identified
- All critical acceptance criteria met with test coverage >90%
- Code quality excellent, follows all standards

**Decision Rationale**:

- All 10 acceptance criteria fully implemented and tested ✅
- Test coverage 92.92% exceeds target (>70%) ✅
- Code quality meets professional standards ✅
- Security appropriate for MVP scope ✅
- Performance characteristics excellent ✅
- Minor refactoring completed during review ✅
- Pre-existing frontend test issue noted but not blocking (Story 3.2 scope) ✅

### Recommended Status

✅ **Ready for Done**

**Justification:**

- All acceptance criteria met with comprehensive test coverage
- Code quality excellent with only minor refactoring needed (completed)
- No blocking issues identified
- Docker configuration validated
- Integration tests pass with real WebSocket connections
- Pre-existing frontend test issue is outside scope (belongs to Story 3.2)

**Next Steps:**

1. Developer updates File List to reflect QA refactoring of `server/index.ts`
2. Story can be moved to "Done" status
3. Frontend test configuration issue should be tracked separately (recommend creating Story 3.2.1 for frontend test fixes)

## Change Log

| Date       | Version | Description                                                                     | Author     |
| ---------- | ------- | ------------------------------------------------------------------------------- | ---------- |
| 2025-12-29 | 1.0     | Initial story draft with comprehensive technical context from architecture docs | BMAD Agent |
